{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/am3lia-low/battle-of-BERTS-FOMC/blob/main/FOMC_Step2_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1299c366",
      "metadata": {
        "id": "1299c366"
      },
      "source": [
        "# FOMC Hawkish-Dovish Classification — Step 2: FinBERT vs RoBERTa Training\n",
        "## DSA4265 Take-Home Assignment\n",
        "\n",
        "This notebook covers:\n",
        "1. Training **FinBERT** (financial domain pre-trained BERT) on FOMC hawkish-dovish classification\n",
        "2. Training **RoBERTa-large** (the original paper's best model) for comparison\n",
        "3. Comprehensive evaluation: confusion matrices, ROC curves, per-class metrics\n",
        "4. Error analysis and model comparison\n",
        "5. Analysis by year/era\n",
        "\n",
        "**Models:**\n",
        "- `yiyanghkust/finbert-pretrain` — BERT pre-trained on financial communications (110M params)\n",
        "- `roberta-large` — General-purpose language model (355M params)\n",
        "\n",
        "**Research Question:** Can financial domain pre-training (FinBERT, 110M params) outperform a larger general-purpose model (RoBERTa, 355M params) on monetary policy stance classification?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "640b7a99",
      "metadata": {
        "id": "640b7a99"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe5f130",
      "metadata": {
        "id": "fbe5f130"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate scikit-learn matplotlib seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8140a662",
      "metadata": {
        "id": "8140a662"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from scipy.special import softmax\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    # print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU. Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb252b60",
      "metadata": {
        "id": "cb252b60"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "\n",
        "df_train = pd.read_csv(DRIVE_PATH + 'train.csv')\n",
        "df_val = pd.read_csv(DRIVE_PATH + 'val.csv')\n",
        "df_test = pd.read_csv(DRIVE_PATH + 'test.csv')\n",
        "\n",
        "LABEL_MAP = {0: 'Dovish', 1: 'Hawkish', 2: 'Neutral'}\n",
        "NUM_LABELS = 3\n",
        "\n",
        "print(f\"Train: {len(df_train):,}  Val: {len(df_val):,}  Test: {len(df_test):,}\")\n",
        "print(f\"\\nTrain label distribution:\")\n",
        "print(df_train['label'].map(LABEL_MAP).value_counts().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64899f9",
      "metadata": {
        "id": "c64899f9"
      },
      "source": [
        "## 2. Dataset & Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9caea1c",
      "metadata": {
        "id": "f9caea1c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FOMCDataset(Dataset):\n",
        "    \"\"\"Dataset for FOMC hawkish-dovish classification.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            str(self.texts[idx]),\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"Dataset class defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e30a22",
      "metadata": {
        "id": "05e30a22"
      },
      "source": [
        "## 3. Training Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a9d19c",
      "metadata": {
        "id": "e4a9d19c"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for Hugging Face Trainer.\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    probs = softmax(logits, axis=1)\n",
        "    labels_bin = label_binarize(labels, classes=[0, 1, 2])\n",
        "    try:\n",
        "        auc_score = roc_auc_score(labels_bin, probs, multi_class='ovr', average='macro')\n",
        "    except ValueError:\n",
        "        auc_score = 0.0\n",
        "    return {'accuracy': accuracy, 'precision_macro': prec, 'recall_macro': rec,\n",
        "            'f1_macro': f1, 'auc_macro': auc_score}\n",
        "\n",
        "\n",
        "def train_and_evaluate(model_name, tokenizer_name, output_name, num_epochs=5,\n",
        "                       learning_rate=2e-5, batch_size=16, max_length=128):\n",
        "    \"\"\"Train a model and return evaluation results.\"\"\"\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING: {output_name}\")\n",
        "    print(f\"  Model: {model_name}\")\n",
        "    print(f\"  Epochs: {num_epochs}, LR: {learning_rate}, Batch: {batch_size}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=NUM_LABELS\n",
        "    ).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  Total params: {total_params:,}\")\n",
        "    print(f\"  Trainable params: {trainable_params:,}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = FOMCDataset(df_train['sentence'].values, df_train['label'].values,\n",
        "                                tokenizer, max_length)\n",
        "    val_dataset = FOMCDataset(df_val['sentence'].values, df_val['label'].values,\n",
        "                              tokenizer, max_length)\n",
        "    test_dataset = FOMCDataset(df_test['sentence'].values, df_test['label'].values,\n",
        "                               tokenizer, max_length)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=DRIVE_PATH + f'{output_name}_checkpoints',\n",
        "        num_train_epochs=num_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=32,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1_macro',\n",
        "        greater_is_better=True,\n",
        "        logging_steps=50,\n",
        "        seed=42,\n",
        "        fp16=True,\n",
        "        report_to='none',\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=training_args,\n",
        "        train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    train_result = trainer.train()\n",
        "    print(f\"  Training time: {train_result.metrics['train_runtime']:.0f}s\")\n",
        "    print(f\"  Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    predictions_output = trainer.predict(test_dataset)\n",
        "    logits = predictions_output.predictions\n",
        "    y_true = predictions_output.label_ids\n",
        "    y_pred = np.argmax(logits, axis=-1)\n",
        "    y_probs = softmax(logits, axis=1)\n",
        "\n",
        "    # Compute all metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "    y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
        "    auc_macro = roc_auc_score(y_true_bin, y_probs, multi_class='ovr', average='macro')\n",
        "\n",
        "    print(f\"\\n  TEST RESULTS:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  F1 Macro:  {f1_macro:.4f}\")\n",
        "    print(f\"  AUC Macro: {auc_macro:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    save_path = DRIVE_PATH + f'{output_name}_model'\n",
        "    trainer.save_model(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    # Collect training history\n",
        "    log_history = trainer.state.log_history\n",
        "\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'output_name': output_name,\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': prec_macro,\n",
        "        'recall_macro': rec_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'auc_macro': auc_macro,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'y_probs': y_probs,\n",
        "        'y_true_bin': y_true_bin,\n",
        "        'log_history': log_history,\n",
        "        'training_time': train_result.metrics['train_runtime'],\n",
        "    }\n",
        "\n",
        "    # Free GPU memory\n",
        "    del model, trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"Training helper function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9269c766",
      "metadata": {
        "id": "9269c766"
      },
      "source": [
        "## 4. Train FinBERT\n",
        "\n",
        "`yiyanghkust/finbert-pretrain` is a BERT-base model further pre-trained on a large corpus of\n",
        "financial communications including corporate reports, earnings call transcripts, and analyst reports.\n",
        "\n",
        "We fine-tune it on the FOMC hawkish-dovish task — a novel application since FinBERT was never\n",
        "trained on central bank language specifically. This tests whether financial domain knowledge\n",
        "transfers to monetary policy text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7508f875",
      "metadata": {
        "id": "7508f875"
      },
      "outputs": [],
      "source": [
        "finbert_results = train_and_evaluate(\n",
        "    model_name='yiyanghkust/finbert-pretrain',\n",
        "    tokenizer_name='yiyanghkust/finbert-pretrain',\n",
        "    output_name='finbert',\n",
        "    num_epochs=5,\n",
        "    learning_rate=2e-5,\n",
        "    batch_size=16,\n",
        "    max_length=128\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf290197",
      "metadata": {
        "id": "cf290197"
      },
      "source": [
        "## 5. Train RoBERTa-large\n",
        "\n",
        "`roberta-large` was the best-performing model in the original Shah et al. (2023) paper.\n",
        "It's a general-purpose language model with 355M parameters — over 3x the size of FinBERT.\n",
        "\n",
        "This serves as our benchmark: can FinBERT's domain knowledge compensate for RoBERTa's\n",
        "larger size and broader pre-training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3f5048",
      "metadata": {
        "id": "aa3f5048"
      },
      "outputs": [],
      "source": [
        "roberta_results = train_and_evaluate(\n",
        "    model_name='roberta-large',\n",
        "    tokenizer_name='roberta-large',\n",
        "    output_name='roberta',\n",
        "    num_epochs=5,\n",
        "    learning_rate=1e-5,  # Lower LR for larger model\n",
        "    batch_size=8,         # Smaller batch for larger model (GPU memory)\n",
        "    max_length=128\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a10269f",
      "metadata": {
        "id": "9a10269f"
      },
      "source": [
        "## 6. Head-to-Head Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70da309f",
      "metadata": {
        "id": "70da309f"
      },
      "outputs": [],
      "source": [
        "# Summary table\n",
        "print(\"MODEL COMPARISON — TEST SET\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Metric':<20} {'FinBERT':<15} {'RoBERTa-large':<15} {'Difference':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro', 'auc_macro']\n",
        "metric_names = ['Accuracy', 'Precision (M)', 'Recall (M)', 'F1 Score (M)', 'AUC-ROC (M)']\n",
        "\n",
        "for metric, name in zip(metrics, metric_names):\n",
        "    fb = finbert_results[metric]\n",
        "    rb = roberta_results[metric]\n",
        "    diff = fb - rb\n",
        "    winner = \"FinBERT\" if diff > 0 else \"RoBERTa\"\n",
        "    print(f\"  {name:<20} {fb:<15.4f} {rb:<15.4f} {diff:+.4f} ({winner})\")\n",
        "\n",
        "print(f\"\\n  {'Parameters':<20} {finbert_results['total_params']:>13,} {roberta_results['total_params']:>13,}\")\n",
        "print(f\"  {'Training Time':<20} {finbert_results['training_time']:>12.0f}s {roberta_results['training_time']:>12.0f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c64869",
      "metadata": {
        "id": "10c64869"
      },
      "outputs": [],
      "source": [
        "# Bar chart comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(metric_names))\n",
        "width = 0.35\n",
        "\n",
        "fb_scores = [finbert_results[m] for m in metrics]\n",
        "rb_scores = [roberta_results[m] for m in metrics]\n",
        "\n",
        "bars1 = ax.bar(x - width/2, fb_scores, width, label='FinBERT (110M)', color='#2ecc71')\n",
        "bars2 = ax.bar(x + width/2, rb_scores, width, label='RoBERTa-large (355M)', color='#e74c3c')\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('FinBERT vs RoBERTa-large — Test Set Performance')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metric_names, rotation=15)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
        "                f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e322b3",
      "metadata": {
        "id": "c6e322b3"
      },
      "source": [
        "## 7. Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4545ffb",
      "metadata": {
        "id": "c4545ffb"
      },
      "outputs": [],
      "source": [
        "label_names = ['Dovish', 'Hawkish', 'Neutral']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for idx, (results, name) in enumerate([(finbert_results, 'FinBERT'), (roberta_results, 'RoBERTa-large')]):\n",
        "    # Raw counts\n",
        "    cm = confusion_matrix(results['y_true'], results['y_pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx][0],\n",
        "                xticklabels=label_names, yticklabels=label_names)\n",
        "    axes[idx][0].set_xlabel('Predicted')\n",
        "    axes[idx][0].set_ylabel('Actual')\n",
        "    axes[idx][0].set_title(f'{name} — Counts')\n",
        "\n",
        "    # Normalized\n",
        "    cm_norm = confusion_matrix(results['y_true'], results['y_pred'], normalize='true')\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.1%', cmap='Blues', ax=axes[idx][1],\n",
        "                xticklabels=label_names, yticklabels=label_names)\n",
        "    axes[idx][1].set_xlabel('Predicted')\n",
        "    axes[idx][1].set_ylabel('Actual')\n",
        "    axes[idx][1].set_title(f'{name} — Normalized')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Per-class detailed report\n",
        "for results, name in [(finbert_results, 'FinBERT'), (roberta_results, 'RoBERTa-large')]:\n",
        "    print(f\"\\n{name} — CLASSIFICATION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(classification_report(results['y_true'], results['y_pred'],\n",
        "                               target_names=label_names, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09aef29",
      "metadata": {
        "id": "a09aef29"
      },
      "source": [
        "## 8. ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f481cc4f",
      "metadata": {
        "id": "f481cc4f"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "colors_roc = ['#3498db', '#e74c3c', '#95a5a6']\n",
        "\n",
        "for ax, (results, name) in zip(axes, [(finbert_results, 'FinBERT'), (roberta_results, 'RoBERTa-large')]):\n",
        "    for i, (label, color) in enumerate(zip(label_names, colors_roc)):\n",
        "        fpr, tpr, _ = roc_curve(results['y_true_bin'][:, i], results['y_probs'][:, i])\n",
        "        roc_auc_val = auc(fpr, tpr)\n",
        "        ax.plot(fpr, tpr, color=color, linewidth=2,\n",
        "                label=f'{label} (AUC = {roc_auc_val:.3f})')\n",
        "\n",
        "    # Macro average\n",
        "    all_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(NUM_LABELS):\n",
        "        fpr, tpr, _ = roc_curve(results['y_true_bin'][:, i], results['y_probs'][:, i])\n",
        "        mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "    mean_tpr /= NUM_LABELS\n",
        "    ax.plot(all_fpr, mean_tpr, 'k--', linewidth=2,\n",
        "            label=f'Macro Avg (AUC = {results[\"auc_macro\"]:.3f})')\n",
        "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title(f'{name} — ROC Curves')\n",
        "    ax.legend(loc='lower right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'roc_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14783642",
      "metadata": {
        "id": "14783642"
      },
      "source": [
        "## 9. Training Dynamics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46aee33",
      "metadata": {
        "id": "c46aee33"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for results, name, color in [(finbert_results, 'FinBERT', '#2ecc71'),\n",
        "                              (roberta_results, 'RoBERTa', '#e74c3c')]:\n",
        "    log = results['log_history']\n",
        "    train_logs = [x for x in log if 'loss' in x and 'eval_loss' not in x]\n",
        "    eval_logs = [x for x in log if 'eval_loss' in x]\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0].plot([x['step'] for x in train_logs], [x['loss'] for x in train_logs],\n",
        "                 alpha=0.5, color=color, label=f'{name} Train')\n",
        "    axes[0].plot([x['step'] for x in eval_logs], [x['eval_loss'] for x in eval_logs],\n",
        "                 marker='o', linewidth=2, color=color, linestyle='--', label=f'{name} Val')\n",
        "\n",
        "    # F1 curves\n",
        "    epochs = list(range(1, len(eval_logs) + 1))\n",
        "    axes[1].plot(epochs, [x['eval_f1_macro'] for x in eval_logs],\n",
        "                 marker='o', linewidth=2, color=color, label=f'{name} F1')\n",
        "    axes[1].plot(epochs, [x['eval_accuracy'] for x in eval_logs],\n",
        "                 marker='s', linewidth=2, color=color, linestyle='--', label=f'{name} Acc')\n",
        "\n",
        "axes[0].set_xlabel('Step'); axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training & Validation Loss'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Score')\n",
        "axes[1].set_title('Validation F1 & Accuracy'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace949b9",
      "metadata": {
        "id": "ace949b9"
      },
      "source": [
        "## 10. Error Analysis\n",
        "\n",
        "Where do the models disagree? Where does one succeed and the other fail?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03335692",
      "metadata": {
        "id": "03335692"
      },
      "outputs": [],
      "source": [
        "# Build error analysis dataframe\n",
        "df_errors = df_test.copy()\n",
        "df_errors['finbert_pred'] = finbert_results['y_pred']\n",
        "df_errors['roberta_pred'] = roberta_results['y_pred']\n",
        "df_errors['finbert_correct'] = df_errors['label'] == df_errors['finbert_pred']\n",
        "df_errors['roberta_correct'] = df_errors['label'] == df_errors['roberta_pred']\n",
        "\n",
        "# Agreement between models\n",
        "both_correct = (df_errors['finbert_correct'] & df_errors['roberta_correct']).sum()\n",
        "both_wrong = (~df_errors['finbert_correct'] & ~df_errors['roberta_correct']).sum()\n",
        "finbert_only = (df_errors['finbert_correct'] & ~df_errors['roberta_correct']).sum()\n",
        "roberta_only = (~df_errors['finbert_correct'] & df_errors['roberta_correct']).sum()\n",
        "\n",
        "print(\"MODEL AGREEMENT ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Both correct:    {both_correct:>4}  ({both_correct/len(df_errors)*100:.1f}%)\")\n",
        "print(f\"  Both wrong:      {both_wrong:>4}  ({both_wrong/len(df_errors)*100:.1f}%)\")\n",
        "print(f\"  FinBERT only:    {finbert_only:>4}  ({finbert_only/len(df_errors)*100:.1f}%)\")\n",
        "print(f\"  RoBERTa only:    {roberta_only:>4}  ({roberta_only/len(df_errors)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc8bf47",
      "metadata": {
        "id": "bbc8bf47"
      },
      "outputs": [],
      "source": [
        "# Examples where FinBERT succeeds but RoBERTa fails\n",
        "fb_wins = df_errors[df_errors['finbert_correct'] & ~df_errors['roberta_correct']]\n",
        "print(f\"FINBERT CORRECT, RoBERTa WRONG ({len(fb_wins)} cases)\")\n",
        "print(\"=\" * 80)\n",
        "for _, row in fb_wins.head(5).iterrows():\n",
        "    true_label = LABEL_MAP[row['label']]\n",
        "    rb_label = LABEL_MAP[row['roberta_pred']]\n",
        "    print(f\"  True: {true_label:<10} RoBERTa: {rb_label:<10}\")\n",
        "    print(f\"  [{row['year']}] {row['sentence'][:120]}...\")\n",
        "    print()\n",
        "\n",
        "# Examples where RoBERTa succeeds but FinBERT fails\n",
        "rb_wins = df_errors[~df_errors['finbert_correct'] & df_errors['roberta_correct']]\n",
        "print(f\"\\nRoBERTa CORRECT, FinBERT WRONG ({len(rb_wins)} cases)\")\n",
        "print(\"=\" * 80)\n",
        "for _, row in rb_wins.head(5).iterrows():\n",
        "    true_label = LABEL_MAP[row['label']]\n",
        "    fb_label = LABEL_MAP[row['finbert_pred']]\n",
        "    print(f\"  True: {true_label:<10} FinBERT: {fb_label:<10}\")\n",
        "    print(f\"  [{row['year']}] {row['sentence'][:120]}...\")\n",
        "    print()\n",
        "\n",
        "# Examples where BOTH fail\n",
        "both_fail = df_errors[~df_errors['finbert_correct'] & ~df_errors['roberta_correct']]\n",
        "print(f\"\\nBOTH WRONG ({len(both_fail)} cases)\")\n",
        "print(\"=\" * 80)\n",
        "for _, row in both_fail.head(5).iterrows():\n",
        "    true_label = LABEL_MAP[row['label']]\n",
        "    fb_label = LABEL_MAP[row['finbert_pred']]\n",
        "    rb_label = LABEL_MAP[row['roberta_pred']]\n",
        "    print(f\"  True: {true_label:<10} FinBERT: {fb_label:<10} RoBERTa: {rb_label:<10}\")\n",
        "    print(f\"  [{row['year']}] {row['sentence'][:120]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "344e852c",
      "metadata": {
        "id": "344e852c"
      },
      "source": [
        "## 11. Performance by Monetary Policy Era\n",
        "\n",
        "Does model accuracy vary across different monetary policy regimes?\n",
        "- **Pre-GFC (1996–2007):** Greenspan era, rate hikes, dot-com bubble\n",
        "- **GFC & Recovery (2008–2015):** Zero rates, QE, forward guidance\n",
        "- **Normalization (2016–2019):** Gradual rate hikes\n",
        "- **COVID & Inflation (2020–2022):** Emergency cuts, then aggressive hikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b84aa27",
      "metadata": {
        "id": "2b84aa27"
      },
      "outputs": [],
      "source": [
        "def assign_era(year):\n",
        "    if year <= 2007: return 'Pre-GFC'\n",
        "    elif year <= 2015: return 'GFC & Recovery'\n",
        "    elif year <= 2019: return 'Normalization'\n",
        "    else: return 'COVID & Inflation'\n",
        "\n",
        "df_errors['era'] = df_errors['year'].apply(assign_era)\n",
        "\n",
        "print(\"PERFORMANCE BY ERA\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Era':<22} {'N':<6} {'FinBERT Acc':<14} {'RoBERTa Acc':<14} {'Gap':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "era_results = {}\n",
        "for era in ['Pre-GFC', 'GFC & Recovery', 'Normalization', 'COVID & Inflation']:\n",
        "    mask = df_errors['era'] == era\n",
        "    if mask.sum() > 0:\n",
        "        fb_acc = df_errors.loc[mask, 'finbert_correct'].mean()\n",
        "        rb_acc = df_errors.loc[mask, 'roberta_correct'].mean()\n",
        "        gap = fb_acc - rb_acc\n",
        "        era_results[era] = {'n': mask.sum(), 'finbert': fb_acc, 'roberta': rb_acc}\n",
        "        print(f\"  {era:<22} {mask.sum():<6} {fb_acc:<14.3f} {rb_acc:<14.3f} {gap:+.3f}\")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "eras = list(era_results.keys())\n",
        "x = np.arange(len(eras))\n",
        "width = 0.35\n",
        "ax.bar(x - width/2, [era_results[e]['finbert'] for e in eras], width,\n",
        "       label='FinBERT', color='#2ecc71')\n",
        "ax.bar(x + width/2, [era_results[e]['roberta'] for e in eras], width,\n",
        "       label='RoBERTa-large', color='#e74c3c')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Model Performance by Monetary Policy Era')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(eras, rotation=15)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'performance_by_era.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f377f165",
      "metadata": {
        "id": "f377f165"
      },
      "source": [
        "## 12. Save All Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fb3321",
      "metadata": {
        "id": "c7fb3321"
      },
      "outputs": [],
      "source": [
        "# Save comprehensive results\n",
        "all_results = {\n",
        "    'finbert': {\n",
        "        'model': 'yiyanghkust/finbert-pretrain',\n",
        "        'params': finbert_results['total_params'],\n",
        "        'accuracy': float(finbert_results['accuracy']),\n",
        "        'precision_macro': float(finbert_results['precision_macro']),\n",
        "        'recall_macro': float(finbert_results['recall_macro']),\n",
        "        'f1_macro': float(finbert_results['f1_macro']),\n",
        "        'auc_macro': float(finbert_results['auc_macro']),\n",
        "        'training_time_s': float(finbert_results['training_time']),\n",
        "    },\n",
        "    'roberta': {\n",
        "        'model': 'roberta-large',\n",
        "        'params': roberta_results['total_params'],\n",
        "        'accuracy': float(roberta_results['accuracy']),\n",
        "        'precision_macro': float(roberta_results['precision_macro']),\n",
        "        'recall_macro': float(roberta_results['recall_macro']),\n",
        "        'f1_macro': float(roberta_results['f1_macro']),\n",
        "        'auc_macro': float(roberta_results['auc_macro']),\n",
        "        'training_time_s': float(roberta_results['training_time']),\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(DRIVE_PATH + 'evaluation_results.json', 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "print(f\"Results saved to: {DRIVE_PATH}evaluation_results.json\")\n",
        "\n",
        "# Save error analysis\n",
        "df_errors.to_csv(DRIVE_PATH + 'error_analysis.csv', index=False)\n",
        "print(f\"Error analysis saved to: {DRIVE_PATH}error_analysis.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ALL DONE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nFinBERT:  Acc={finbert_results['accuracy']:.4f}  F1={finbert_results['f1_macro']:.4f}  AUC={finbert_results['auc_macro']:.4f}\")\n",
        "print(f\"RoBERTa:  Acc={roberta_results['accuracy']:.4f}  F1={roberta_results['f1_macro']:.4f}  AUC={roberta_results['auc_macro']:.4f}\")\n",
        "print(\"\\nReady for report writing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aab39da",
      "metadata": {
        "id": "2aab39da"
      },
      "source": [
        "## 13. Summary\n",
        "\n",
        "### Models Trained\n",
        "1. **FinBERT** (`yiyanghkust/finbert-pretrain`) — 110M params, financial domain pre-training\n",
        "2. **RoBERTa-large** (`roberta-large`) — 355M params, general-purpose, original paper's best model\n",
        "\n",
        "### Research Question\n",
        "Can FinBERT's financial domain knowledge compensate for RoBERTa's 3x larger size on monetary policy stance classification?\n",
        "\n",
        "### Key Outputs\n",
        "- `model_comparison.png` — head-to-head metric comparison\n",
        "- `confusion_matrices.png` — per-model confusion matrices\n",
        "- `roc_curves.png` — ROC curves for both models\n",
        "- `training_curves.png` — loss and metric convergence\n",
        "- `performance_by_era.png` — accuracy across monetary policy regimes\n",
        "- `evaluation_results.json` — all metrics\n",
        "- `error_analysis.csv` — per-sentence predictions for analysis\n",
        "\n",
        "### Next Step\n",
        "- **Step 3:** Write the report"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}